# 仮説①検証レポート：実装・評価のミスマッチについて

## 結論
**仮説は正しい。**
特に「Metric計算ロジック」に重大な実装ミスがあり、これがCVスコアの天井（0.61付近）の原因となっていた可能性が極めて高い。

## 検証結果サマリ

| 検証項目 | 結果 | 詳細 |
| :--- | :--- | :--- |
| **Albumentations Resize** | ✅ 問題なし | マスク画像に対しては自動的に Nearest Neighbor 補間が適用されており、値の破壊は起きていない。 |
| **Overfitting能力** | ✅ 問題なし | 修正後のMetricを用いた場合、わずか数枚の学習データに対して mIoU > 0.85 まで容易に到達することを確認。モデル自体の学習能力に問題はない。 |
| **Metric実装** | ❌ **バグあり** | `IoUMetric` の実装において、**「正解ラベルに存在しないクラス（Union=0）」も含めて平均をとっていた**ため、スコアが不当に低く算出されていた。 |

## 詳細分析

### 1. Metric実装の不備
既存の `IoUMetric` クラスは以下のように実装されていました：

```python
# 修正前
miou = iou.mean().item()
```

この実装では、そのバッチ（またはデータセット全体）に含まれないクラスについても `IoU = 0` として平均計算に含まれてしまいます。
例えば、ある画像に「壁」「床」「ベッド」しか写っていない場合、その他の「椅子」「窓」などのクラスは Union=0 となりますが、これらが 0点 として加算され、全体の平均を大きく押し下げていました。

**修正案:**
Unionが0より大きいクラスのみで平均をとるように変更する必要があります。

```python
# 修正後
valid_classes = union > 0
if valid_classes.sum() == 0:
    return 0.0
miou = iou[valid_classes].mean().item()
```

### 2. Overfitting テストの結果
Metric修正前後での Overfitting テスト（4枚の画像で学習）の結果比較：

- **修正前**: Loss は順調に下がる (0.06) が、mIoU は **0.51** 付近で頭打ち。
- **修正後**: Loss の低下に伴い、mIoU は **0.85** を超えて上昇（学習継続でさらに向上する傾向）。

これにより、モデル自体は正常に学習できているものの、評価指標が実力を正しく反映できていなかったことが確定しました。

## 次のアクション
1.  **本番コードの修正**: `src/pipeline-9.py` (および他のパイプライン) の `IoUMetric` クラスを修正する。
2.  **再評価**: 修正した Metric で再度学習・評価を行い、CVスコアが LB と相関するか、あるいは 0.75 などの高い値が出るかを確認する。
